{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"11BXjiS26N9-7wzPlwcwKew2v6BikpW51","authorship_tag":"ABX9TyMHa8ivNl0v1w78fBXGpcEP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Mount the google drive "],"metadata":{"id":"ZGoX3SVXj3dR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"w7QF1pY5jplc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674660181278,"user_tz":300,"elapsed":19934,"user":{"displayName":"fatima ronda","userId":"15031632683178563098"}},"outputId":"5b55872c-f0f1-454f-fb62-64dd47ecfe31"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["Building the training datasets"],"metadata":{"id":"3p8bAQdjkeRk"}},{"cell_type":"code","source":["!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials# Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","import tensorflow as tf\n","import numpy as np # linear algebra\n","import os\n","import sys\n","import time\n","import gc\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import torch\n","import datetime\n","\n","torch.autograd.set_detect_anomaly(False)\n","torch.autograd.profiler.profile(False)\n","torch.autograd.profiler.emit_nvtx(False)\n","\n","sys.path.append('/content/drive/MyDrive/AdaVFL-GitHub')\n","from Data import MakeTrainingTimes, CheckLocalTrainData, CheckLocalPredictionData, GenerateRandomSamples, LocalSequentialDataset, LocalSampledDataset, GlobalSequentialDataset, GlobalSampledDataset, build_adjMatrix, MakeAttackTimes\n","from federationarguments import arguments\n","\n","args = arguments()\n","# args = arguments(\"Yelp\",8,8)\n","if(args.dataset == \"bikeNYC\"):\n","  link = \"https://drive.google.com/drive/folders/1diJwebRNa5AQ16Jy6eHNGtYGmIGeqcrt\"\n","  args.trainingInterval = 24*60*60*1000\n","  args.x_axis = 8\n","  args.y_axis = 16\n","else:\n","  if(args.dataset == \"Yelp\"):\n","    link = \"https://drive.google.com/drive/folders/1K2Y_txKAda0TOEEDYvoa7sPPMYLCXI-U\"\n","    args.trainingInterval = 8*24*60*60*1000\n","    args.x_axis = 8\n","    args.y_axis = 8\n","  else:\n","    raise SystemError('Invalid data folder') \n","\n","device_name = tf.test.gpu_device_name()\n","if device_name == '/device:GPU:0':\n","  print('Found GPU at: {}'.format(device_name))\n","  args.device_name = 'cuda'\n","  torch.cuda.set_device(0)\n","else:\n","  args.device_name = 'cpu'\n","  # raise SystemError('GPU device not found')\n","\n","fluff, folder = link.split('folders/')\n","#print (id) \n","filePath = \"'%s' in parents and trashed=false\" %folder\n","print(filePath)\n","downloaded = drive.ListFile({'q':filePath}).GetList()\n","\n","LocalTrainData = []\n","LocalValidationData = []\n","LocalTestData = []\n","LocalPredictionSamples = {}\n","GlobalPredictionSamples = GlobalSampledDataset(args)\n","GlobalTestData = GlobalSequentialDataset(args,args.testRatioBegin,args.testRatioEnd)\n","AttackerTrainData = GlobalSequentialDataset(args,args.trainAttackerBegin,args.trainAttackerEnd)\n","attacker_test_dataset = GlobalSampledDataset(args)\n","\n","sampled_list = []\n","sampledPrediction = True\n","if len(downloaded) > 0:\n","  sampledPrediction = True\n","  for file in downloaded:\n","    try:\n","      downloaded = drive.CreateFile({'id':file['id']}) \n","      downloaded.GetContentFile(file['title'])  \n","      timestamps = pd.read_csv(file['title'])\n","      grid = file['title'].split(\".\")\n","      axis = grid[0].split(\"X\")\n","      x_axis = int(axis[0])\n","      y_axis = int(axis[1])\n","      train = LocalSequentialDataset(timestamps,x_axis, y_axis, args.trainRatioBegin,args.trainRatioEnd, args)\n","      train.make_data()\n","      LocalTrainData.append(train)\n","\n","      if(sampledPrediction == True):\n","          sampled_list = GenerateRandomSamples(timestamps, args)    \n","          sampledPrediction = False  \n","\n","      predictsample = LocalSampledDataset(timestamps, x_axis, y_axis, args)\n","      predictsample.make_data(sampled_list)\n","      LocalValidationData.append(predictsample)\n","      LocalValidationData.append(predictsample)\n","      test = LocalSequentialDataset(timestamps,x_axis, y_axis, args.testRatioBegin,args.testRatioEnd, args)\n","      test.make_data()\n","      LocalTestData.append(test)\n","      sample_ID = str(x_axis)+\"X\"+str(y_axis)\n","      LocalPredictionSamples[sample_ID] = predictsample\n","      GlobalTestData.add_data(file['title'], timestamps)\n","      GlobalPredictionSamples.add_data(file['title'], predictsample)\n","      AttackerTrainData.add_data(file['title'], timestamps)\n","      attacker_test_dataset.add_data(file['title'], predictsample)\n","    except Exception as e:\n","        print(\"hit an exception when making data \",e)\n","        exit('hit an exception when making data')    \n","\n","GlobalTestData.make_data()\n","GlobalTestData.check_data()\n","\n","AttackerTrainData.make_data()\n","AttackerTrainData.check_data()\n","\n","MakeAttackTimes(AttackerTrainData,args)\n","MakeTrainingTimes(LocalTrainData, args)\n","\n","CheckLocalTrainData(LocalTrainData, args)\n","\n","CheckLocalPredictionData(LocalPredictionSamples, sampled_list)\n","\n","GlobalPredictionSamples.make_data()\n","GlobalPredictionSamples.check_data()\n","\n","attacker_test_dataset.make_data()\n","attacker_test_dataset.check_data()\n","print(\"len(LocalPredictionSamples)\",len(LocalPredictionSamples))\n","\n","adj = build_adjMatrix(args)\n","beginningTime = args.beginTrainingTimestamp\n","endingTime = args.endTrainingTimestamp\n","iterations = args.epochs*(((endingTime - beginningTime)//args.trainingInterval)+1)\n","\n","print(\"starting training time\", datetime.datetime.fromtimestamp(args.start_training_time/1000).strftime('%Y-%m-%d %H:%M:%S'))\n","print(\"ending training time \", datetime.datetime.fromtimestamp(args.start_ending_time/1000).strftime('%Y-%m-%d %H:%M:%S'))\n","\n","del sampled_list\n","del timestamps\n","gc.collect()\n"],"metadata":{"id":"W5NARITKkgxj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674660312029,"user_tz":300,"elapsed":69945,"user":{"displayName":"fatima ronda","userId":"15031632683178563098"}},"outputId":"428ceb0f-0656-466b-8834-3389a48ab902"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'1K2Y_txKAda0TOEEDYvoa7sPPMYLCXI-U' in parents and trashed=false\n","GlobalSequentialDataset:size of seqDict 434\n","GlobalSequentialDataset:size of labelDict 434\n","GlobalSequentialDataset min value tensor(-1.)\n","GlobalSequentialDataset max value tensor(-1.)\n","start timestamp of global sequential 1599908399000\n","end timestamp of global sequential 1609433999000\n","GlobalSequentialDataset:size of seqDict 868\n","GlobalSequentialDataset:size of labelDict 868\n","GlobalSequentialDataset min value tensor(-1.)\n","GlobalSequentialDataset max value tensor(-1.)\n","start timestamp of global sequential 1562108399000\n","end timestamp of global sequential 1581008399000\n","CheckLocalTrainData\n","beginningTime= 1514782799000\n","endingTime= 1538197199000\n","args.trainingInterval= 691200000\n","CheckLocalPredictionData\n","size of seqDict 657\n","GlobalSampledDataset\n","GlobalSampledDataset:size of seqDict 657\n","GlobalSampledDataset min value -1.0\n","GlobalSampledDataset max value 1.0\n","GlobalSampledDataset\n","GlobalSampledDataset:size of seqDict 657\n","GlobalSampledDataset min value -1.0\n","GlobalSampledDataset max value 1.0\n","len(LocalPredictionSamples) 64\n","starting training time 2020-09-12 10:59:59\n","ending training time  2020-12-31 16:59:59\n"]},{"output_type":"execute_result","data":{"text/plain":["791"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["Run the privacy attack"],"metadata":{"id":"rXL7XvcsmWmY"}},{"cell_type":"code","source":["import os\n","from Models import GRU, MyGAT\n","from tqdm import tqdm\n","from Data import build_map,build_adjMatrix\n","from WeightTools import train_global_weights, test_model\n","import random \n","from Data import compute_feature_contribution,initialize_ratio_contribution \n","from Initial_feature_weights import initialize_avg_computed_features\n","from Models import MLP\n","from tqdm import tqdm\n","from Data import modify_input_attacker, build_attacker_input, extract_true_input, initialize_ratio_outputsize, train_attacker\n","import datetime\n","import numpy as np\n","import random\n","from torch.optim.lr_scheduler import ExponentialLR\n","\n","###########################\n","# upload the trained global model\n","adj = adj.to(args.device_name)\n","global_test_output=[]\n","\n","globalModel = None\n","if (args.global_model == 'GNN'):\n","  globalModel = MyGAT(args,adj)\n","  globalModel.to(args.device_name)\n","else:\n","  exit('Error: unrecognized global model')\n","\n","print(\"The feature mode to be tested \",args.featureMode)\n","ratio = args.featureRatio\n","\n","if args.dataset == \"Yelp\":\n","  directory_name = \"/content/drive/MyDrive/Colab Notebooks/Models/Yelp/\"\n","else:\n","  directory_name = \"/content/drive/MyDrive/Colab Notebooks/Models/bikenyc/\"\n","\n","if args.featureMode== \"None\":\n","  file_name = \"NoPrivacyGAN.pth\"\n","else:\n","  if args.featureMode== \"Uniform\":\n","    file_name = \"UniformPrivacyGAN.pth\"\n","  else:\n","    if args.featureMode== \"InvContribution\":\n","      file_name = 'InvConPrivacyGAN{}.pth'.format(ratio)\n","    else:\n","      if args.featureMode== \"LinContribution\":\n","        file_name = 'LinConPrivacyGAN{}.pth'.format(ratio)\n","        \n","print(\"The target model's to upload \",file_name)\n","\n","path = '{}{}'.format(directory_name,file_name)\n","map_location='cpu'\n","globalModel.load_state_dict(torch.load(path,map_location=map_location))\n","globalModel.eval()\n","\n","########################### \n","#initialize the attacker's model\n","feature_contribution_metrics = initialize_avg_computed_features(args)\n","organization_contribution_distribution = initialize_ratio_contribution(args,feature_contribution_metrics)\n","feature_contribution_metrics = compute_feature_contribution(organization_contribution_distribution,feature_contribution_metrics,args)\n","attacker = MLP(args,initialize_ratio_outputsize(args))\n","attacker.to(args.device_name)\n","attacker.train()\n","\n","###########################\n","#initialize variables\n","beginningTime = args.beginAttackTimestamp\n","endingTime = args.endAttackTimestamp-args.trainingInterval\n","iterations = ((endingTime - beginningTime)//args.trainingInterval)+1\n","\n","batch_loss = []\n","batch_MAE = []\n","batch_AE = []\n","batch_RMSE = []\n","batch_WMAPE = []\n","\n","###########################\n","#train the attacker's model\n","organization_contribution_distribution = np.tile(organization_contribution_distribution,(args.input_length,1,1))  \n","flat_organization_contribution_distribution = np.copy(organization_contribution_distribution)\n","flat_organization_contribution_distribution = np.ndarray.flatten(flat_organization_contribution_distribution)\n","for epoch in tqdm(range(args.attacker_epochs)):\n","#     for epoch in tqdm(range(10)):\n","# while loss > 0.19:\n","#       for i in tqdm(range(iterations)):\n","  for i in range(iterations):\n","    timestamp = random.randrange(beginningTime, endingTime, args.trainingInterval)\n","    accuracy_training, loss = train_attacker(attacker,timestamp,AttackerTrainData,globalModel,adj,organization_contribution_distribution, args)\n","#         print(\"batch loss\",loss)\n","    batch_loss.append(loss)\n","    batch_MAE.append(accuracy_training[\"MAE\"])\n","    batch_RMSE.append(accuracy_training[\"RMSE\"])\n","    batch_AE.append(accuracy_training[\"AE\"])\n","    batch_WMAPE.append(accuracy_training[\"WMAPE\"])\n","#         print(\"batch accuracy_training[AE]\",accuracy_training[\"AE\"])\n","#         print(\"batch accuracy_training[RMSE]\",accuracy_training[\"RMSE\"])\n","\n","###########################\n","#test the attacker model and output metrics\n","attacker.eval()\n","acc = {}\n","acc[\"MSE\"] = 0.\n","acc[\"MAE\"] = 0.\n","acc[\"RMSE\"] = 0.\n","with torch.no_grad():\n","  predictions = []  \n","  attacker_accuracy = []\n","#       MSE_accuracy = 0.      \n","  sequence_number = 0\n","  for seq, labels in attacker_test_dataset.sequence:\n","    accuracy, predictions= attacker.guess(seq,flat_organization_contribution_distribution,args.attacker_organization_id)\n","    sequence_number += 1\n","    acc[\"MSE\"] += accuracy[\"MSE\"]  \n","    acc[\"RMSE\"] += np.sqrt(accuracy[\"MSE\"]) \n","    acc[\"MAE\"] += accuracy[\"MAE\"] \n","  acc[\"MSE\"] /= sequence_number\n","  acc[\"RMSE\"] /= sequence_number\n","  acc[\"MAE\"] /= sequence_number\n","  print(\"the attacker success MSE=\", acc[\"MSE\"], \",RMSE=\",acc[\"RMSE\"],\",MAE=\",acc[\"MAE\"])"],"metadata":{"id":"WpvBiNUWmfor","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674661537326,"user_tz":300,"elapsed":1143784,"user":{"displayName":"fatima ronda","userId":"15031632683178563098"}},"outputId":"f56d5ae8-a27c-4a1f-8965-64788c1f560d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The feature mode to be tested  LinContribution\n","The target model's to upload  LinConPrivacyGAN90.pth\n","initialize_avg_computed_features: Yelp\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6/6 [18:30<00:00, 185.09s/it]\n"]},{"output_type":"stream","name":"stdout","text":["the attacker success MSE= tensor(1.2575) ,RMSE= tensor(1.1208) ,MAE= tensor(-0.5424)\n"]}]}]}
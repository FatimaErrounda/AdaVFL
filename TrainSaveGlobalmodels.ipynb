{"cells":[{"cell_type":"markdown","metadata":{"id":"OdHgwF8u_LvP"},"source":["**This notebook server to train and save a glovel model**"]},{"cell_type":"markdown","source":["Mount the google drive and authorizations"],"metadata":{"id":"yTmZ8RdD62Dr"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials# Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wdPh-HrJ62qw","executionInfo":{"status":"ok","timestamp":1674758596865,"user_tz":300,"elapsed":20848,"user":{"displayName":"fatima ronda","userId":"15031632683178563098"}},"outputId":"efea8990-972f-46de-d969-f075e1f97e59"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["Common imports and setting up code source paths"],"metadata":{"id":"TlGrgAb_cZon"}},{"cell_type":"code","source":["import numpy as np # linear algebra\n","import os\n","import sys\n","import time\n","import gc\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import torch\n","import datetime\n","####################################\n","# Turning off the debug options to speed up the execution\n","# torch.autograd.set_detect_anomaly(False)\n","# torch.autograd.profiler.profile(False)\n","# torch.autograd.profiler.emit_nvtx(False)\n","\n","sys.path.append('/content/drive/MyDrive/AdaVFL-GitHub')"],"metadata":{"id":"OeS7RJHWcVwV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Upload the data first"],"metadata":{"id":"5O45R91l69hU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QDYLJEj6_KvL"},"outputs":[],"source":["import tensorflow as tf\n","from Data import MakeTrainingTimes, CheckLocalPredictionData, GenerateRandomSamples, LocalSequentialDataset, LocalSampledDataset, GlobalSequentialDataset, GlobalSampledDataset, build_adjMatrix, MakeAttackTimes, initialize_ratio_contribution\n","from federationarguments import arguments\n","\n","args = arguments()\n","if(args.dataset == \"bikeNYC\"):\n","  args.trainingInterval = 24*60*60*1000\n","  args.x_axis = 8\n","  args.y_axis = 16\n","else:\n","  if(args.dataset == \"Yelp\"):\n","    args.trainingInterval = 8*24*60*60*1000\n","    args.x_axis = 8\n","    args.y_axis = 8\n","  else:\n","    raise SystemError('Invalid data folder')  \n","\n","device_name = tf.test.gpu_device_name()\n","if device_name == '/device:GPU:0':\n","  print('Found GPU at: {}'.format(device_name))\n","  args.device_name = 'cuda'\n","  torch.cuda.set_device(0)\n","else:\n","  args.device_name = 'cpu'\n","\n","fluff, folder = args.dataset_link.split('folders/')\n","filePath = \"'%s' in parents and trashed=false\" %folder\n","print(filePath)\n","downloaded = drive.ListFile({'q':filePath}).GetList()\n","\n","LocalTrainData = []\n","LocalPredictionSamples = {}\n","GlobalPredictionSamples = GlobalSampledDataset(args)\n","GlobalTestData = GlobalSequentialDataset(args,args.testRatioBegin,args.testRatioEnd)\n","GlobalTrainData = GlobalSequentialDataset(args,args.trainRatioBegin,args.trainRatioEnd)\n","\n","sampled_list = []\n","\n","if len(downloaded) > 0:\n","  sampledPrediction = True\n","  for file in downloaded:\n","    try:\n","      downloaded = drive.CreateFile({'id':file['id']}) \n","      downloaded.GetContentFile(file['title'])  \n","      timestamps = pd.read_csv(file['title'])\n","      grid = file['title'].split(\".\")\n","      axis = grid[0].split(\"X\")\n","      x_axis = int(axis[0])\n","      y_axis = int(axis[1])\n","      \n","      train = LocalSequentialDataset(timestamps,x_axis, y_axis, args.trainRatioBegin,args.trainRatioEnd, args)\n","      train.make_data()\n","      LocalTrainData.append(train)\n","\n","      if(sampledPrediction == True):\n","        sampled_list = GenerateRandomSamples(timestamps, args)    \n","        sampledPrediction = False  \n","      \n","      predictsample = LocalSampledDataset(timestamps, x_axis, y_axis, args)\n","      predictsample.make_data(sampled_list)\n","      sample_ID = str(x_axis)+\"X\"+str(y_axis)\n","      LocalPredictionSamples[sample_ID] = predictsample\n","      GlobalTestData.add_data(file['title'], timestamps)\n","      GlobalTrainData.add_data(file['title'], timestamps)\n","      GlobalPredictionSamples.add_data(file['title'], predictsample)\n","    except Exception as e:\n","      print(\"hit an exception when making data \",e)\n","      exit('hit an exception when making data')\n","else:\n","  raise SystemError('empty data folder')      \n","\n","GlobalTestData.make_data()\n","print(\"--- Checking global test data ---- \")\n","GlobalTestData.check_data()\n","\n","GlobalTrainData.make_data()\n","print(\"--- Checking local test data ---- \")\n","GlobalTrainData.check_data()\n","\n","print(\"--- Checking local prediction data ---- \")\n","CheckLocalPredictionData(LocalPredictionSamples, sampled_list)\n","\n","print(\"--- Checking global prediction data ---- \")\n","GlobalPredictionSamples.make_data()\n","GlobalPredictionSamples.check_data()\n","print(\"len(LocalPredictionSamples)\",len(LocalPredictionSamples))\n","\n","## TODO: need to set the attacker start time and end time\n","MakeTrainingTimes(LocalTrainData, args)\n","print(\"--- making the timestamp training bouadries ---- \")\n","print(\"args.beginTrainingTimestamp\",args.beginTrainingTimestamp)\n","print(\"args.endTrainingTimestamp\",args.endTrainingTimestamp)\n","\n","adj = build_adjMatrix(args)\n","adj = adj.to(args.device_name)\n","beginningTime = args.beginTrainingTimestamp\n","endingTime = args.endTrainingTimestamp\n","iterations = ((endingTime - beginningTime)//args.trainingInterval)+1\n","\n","print(\"starting training time\", datetime.datetime.fromtimestamp(args.start_training_time/1000).strftime('%Y-%m-%d %H:%M:%S'))\n","print(\"ending training time \", datetime.datetime.fromtimestamp(args.start_ending_time/1000).strftime('%Y-%m-%d %H:%M:%S'))\n","\n","del sampled_list\n","del timestamps\n","gc.collect()\n","print(adj.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"Bza34WGEAIfZ"},"source":["**Only when calculating the metric contribution for each dataset:**\n","* Pre-train a model to use for the metric calculation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VH5YXHmnAIuL"},"outputs":[],"source":["from Models import GRU, MyGAT\n","from tqdm import tqdm\n","import copy\n","from Data import build_map,build_adjMatrix\n","from WeightTools import train_global_weights, test_model\n","import datetime\n","import ast \n","import torchvision.models as models\n","import random \n","\n","adj = adj.to(args.device_name)\n","beginningTime = args.beginTrainingTimestamp\n","endingTime = args.endTrainingTimestamp\n","global_test_output=[]\n","\n","globalModel = None\n","if (args.global_model == 'GNN'):\n","  globalModel = MyGAT(args,adj)\n","  globalModel.to(args.device_name)\n","  globalModel.train()\n","else:\n","  exit('Error: unrecognized global model')\n","\n","start_time = time.time()\n","global_training_loss = []\n","weight_dict = {}\n","number_of_training_rounds = 0\n","\n","global_loss_epoch =10\n","# while global_loss_epoch > 0.1:\n","for epoch in tqdm(range(args.pretrainepochs)):\n","  beginningTime = args.beginTrainingTimestamp + epoch*args.trainingInterval\n","  for i in tqdm(range(iterations)):\n","    timestamp = random.randrange(beginningTime, endingTime, args.trainingInterval)\n","    _,global_loss_epoch = train_global_weights(timestamp, globalModel,GlobalTrainData,args, adj)      \n","    print(\"global_loss_epoch:\",global_loss_epoch)\n","    global_training_loss.append(global_loss_epoch)\n","    gc.collect()\n","global_loss_training = sum(global_training_loss) / len(global_training_loss)\n","\n","### testing the global model:    \n","globalModel.eval() \n","\n","with torch.no_grad():\n","  global_test_output, test_acc, test_loss = test_model(globalModel, adj, GlobalTestData,args)    \n","  now = datetime.datetime.now()\n","  adding_line = '[\\'{}\\',{},\\'{}\\',\\'{}\\',\\'{}\\',\\'{}\\',{},{},{},\\'{}\\',{},{},{},{},{},{},{},{},{}]'.\\\n","      format(args.dataset, args.input_length,args.local_model,args.local_model_loss, args.global_model,args.global_model_loss, args.epochs, args.x_axis, args.y_axis, \n","              now.strftime(\"%Y-%m-%d %H:%M:%S\"), test_acc[\"MAE\"],test_acc[\"RMSE\"],\n","              test_acc[\"AE\"],test_acc[\"WMAPE\"],args.scratch_prediction,args.test_normalization,args.train_normalization, \n","              test_loss,global_loss_training)\n","  print(f' \\n Results after {args.epochs} global rounds of training:',adding_line)"]},{"cell_type":"markdown","metadata":{"id":"J1KnzVrFlA0A"},"source":["**Use the pretrained model to compute the contribution**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_BKHwy-Lk8pE"},"outputs":[],"source":["from tqdm import tqdm\n","import copy\n","\n","from Data import RandomFeature, marginal\n","from WeightTools import predict_model \n","\n","feature_contribution_metrics = [[]]\n","feature_contribution_metrics = np.full((args.x_axis,args.y_axis), -1.) \n","with torch.no_grad():\n","#choose feature\n","  for i in range(0,args.x_axis):\n","    for j in range(0,args.y_axis):\n","      print(\"processing \",i,\" and \",j)\n","      max_marginal = -1.0\n","      for sample in GlobalTestData.sequence:\n","        #calculate the metric:\n","        seq,label = sample\n","        #calculate the model's output\n","        original_output = predict_model(globalModel,seq,adj,args)\n","        #replace the feature with random values\n","        ranseq = RandomFeature(seq, i,j,args)\n","        random_output = predict_model(globalModel,ranseq,adj,args)\n","        marginal_contribution = marginal(random_output,original_output)\n","        if(marginal_contribution > max_marginal):\n","          max_marginal = marginal_contribution\n","      feature_contribution_metrics[i][j] = max_marginal\n","\n","for i in range(0,args.x_axis):\n","  for j in range(0,args.y_axis):\n","    print(\"feature_contribution_metrics[i][j]:\",feature_contribution_metrics[i][j])"]},{"cell_type":"markdown","metadata":{"id":"_CH5V8ezNE1h"},"source":["**build the feature contribution**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2kdYvkFENICD"},"outputs":[],"source":["import sys\n","sys.path.append('/content/drive/MyDrive/AdaVFL-GitHub')\n","from Data import compute_feature_contribution,initialize_ratio_contribution\n","\n","organization_contribution_distribution = initialize_ratio_contribution(args,feature_contribution_metrics)\n","feature_contribution_metrics = compute_feature_contribution(organization_contribution_distribution,feature_contribution_metrics,args)\n","for i in range(0,args.x_axis):\n","  for j in range(0,args.y_axis):\n","    print(\"feature_contribution_metrics[i][j]:\",feature_contribution_metrics[i][j])"]},{"cell_type":"markdown","source":["**Initialize the budgets**"],"metadata":{"id":"taotauwbEL2t"}},{"cell_type":"code","source":["from Data import generate_local_prediction\n","from Models import GRU\n","import random \n","from WeightTools import (rho_to_sigma, sigma_to_rho,compute_advcomp_sigma, compute_advcomp_budget, \n","                         rho_to_dp,compute_cumulated_budget, output_results, update_weights, pertub_weights, \n","                         update_global_weights, Pair, test_model, update_budget_training, \n","                         update_budget_accuracy , update_budget_increase, calculate_validation_accuracy,  \n","                         dp_to_zcdp, grad_func, noisyMax, perturb_gradients, compute_epsilon,\n","                         build_candidates, loss_score, override_model, grad_avg, sigma_to_epsilon, epsilon_to_sigma)\n","\n","\n","args.tracked_error = []  \n","privacy_budgets = []\n","for data in LocalTrainData:\n","    if args.PrivacyMode != \"None\":\n","      total_epsilon = args.epsilon_0\n","      total_delta = args.delta_0\n","      total_rho = dp_to_zcdp(args.epsilon_0,args.delta_0)\n","      rho_t = total_rho/iterations\n","      if args.featureMode == \"Uniform\":\n","          ## initialize with advanced composition\n","          total_epsilon = args.epsilon_1\n","          total_delta = args.delta_0\n","          total_rho = dp_to_zcdp(total_epsilon,total_delta)\n","          rho_t = total_rho/iterations  \n","          epsilon_t, delta_t = compute_advcomp_budget(total_epsilon,total_delta,iterations)\n","          sigma_t = compute_advcomp_sigma(total_epsilon,total_delta,iterations)\n","          privacy_budgets.append(sigma_t)\n","          ## initialize with naive composition \n","          # initial_budget = args.epsilon_0/iterations\n","          # initial_delta = args.delta_0/iterations\n","          # initial_sigma = compute_sigma(initial_budget,initial_delta)\n","      else:\n","        if args.featureMode == \"InvContribution\":\n","          alpha = (1./args.epsilon_0) - (1./args.epsilon_1)\n","          beta = 1./(args.epsilon_1+1.-(args.epsilon_1/args.epsilon_0))\n","          total_epsilon = 1./((alpha*feature_contribution_metrics[data.x_axis][data.y_axis])+((1.-alpha)*beta))\n","          total_delta = args.delta_0/feature_contribution_metrics[data.x_axis][data.y_axis]\n","          total_rho = dp_to_zcdp(total_epsilon,total_delta)\n","          rho_t = total_rho/iterations\n","          sigma_t = rho_to_sigma(rho_t)\n","          delta_t = total_delta/iterations\n","          epsilon_t = rho_to_dp(sigma_t,delta_t)\n","          privacy_budgets.append(sigma_t)\n","          print(\"initial_sigma\",sigma_t)\n","        else:\n","          if args.featureMode == \"LinContribution\":\n","            beta = args.epsilon_1\n","            alpha = args.epsilon_0-beta\n","            total_epsilon = (alpha*feature_contribution_metrics[data.x_axis][data.y_axis])+beta\n","            total_delta = args.delta_0/feature_contribution_metrics[data.x_axis][data.y_axis]\n","            total_rho = dp_to_zcdp(total_epsilon,total_delta)\n","            rho_t = total_rho/iterations\n","            sigma_t = rho_to_sigma(rho_t)\n","            delta_t = total_delta/iterations\n","            epsilon_t = rho_to_dp(sigma_t,delta_t)\n","            privacy_budgets.append(sigma_t)\n","            print(\"initial_sigma\",sigma_t)\n","          else:  \n","            print(\"a problem initializing the privacy budget \")\n","            exit('a problem initializing the privacy budget ')\n","#       print(\"initial_budget\", epsilon_t)\n","#       print(\"initial_delta\",delta_t)\n","#       print(\"initial_sigma\",sigma_t)\n","#       print(\"total_rho\",total_rho)\n","for budget in privacy_budgets:\n","    print(\"Assigned budget\",budget)"],"metadata":{"id":"ysT4FxUQEMSk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0gEfNSwblIFQ"},"source":["**Train the model that will be tested against the privacy attack**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lh02XYTulNkQ"},"outputs":[],"source":["from Models import  GRU, MyGAT\n","from tqdm import tqdm\n","import copy\n","import random \n","from Data import generate_local_prediction, build_map,build_adjMatrix\n","from WeightTools import (output_results, update_weights, pertub_weights, update_global_weights, Pair, \n","                         test_model, update_budget_training, update_budget_accuracy , update_budget_increase, \n","                         calculate_validation_accuracy, dp_to_zcdp, grad_func, noisyMax, perturb_gradients, \n","                         compute_epsilon, build_candidates, loss_score, override_model, grad_avg, sigma_to_epsilon, epsilon_to_sigma)\n","\n","import datetime\n","import ast\n","import torch.multiprocessing as mp\n","\n","device_models = []\n","global_test_output=[]\n","minimum_training_accuracy = []\n","Loop_accuracy = []\n","\n","for data in LocalTrainData:\n","  if(args.local_model == 'GRU'):\n","    model = GRU(args)\n","    #for the parallelism\n","    model.share_memory() \n","    model.train()\n","  else:\n","    exit('Error: unrecognized local model')  \n","  device_models.append(model)\n","\n","# instantiate the global model to train:\n","if(args.global_model == 'GNN'):\n","  globalModel = MyGAT(args,adj) \n","  globalModel.train()\n","else:\n","  exit('Error: unrecognized global model')\n","\n","# initialize variables for the training:\n","start_time = time.time()\n","global_training_loss = []\n","global_training_accuracy_loss_RMSE = []\n","global_training_accuracy_loss_WMAPE = []\n","global_training_accuracy_loss_AE = []\n","local_training_accuracy = [[]]\n","local_training_accuracy = np.full((len(device_models),iterations),0.)\n","local_training_loss = [[]]\n","local_training_loss = np.full((len(device_models),iterations),0.)\n","average_local_training_loss = []\n","average_local_training_loss = np.full(len(device_models),0.)\n","## device side, training locally\n","weight_dict = {}\n","assigned_budget = [[]]\n","assigned_budget = np.full((len(device_models),iterations),0.)\n","minimum_training_accuracy = []\n","minimum_training_accuracy = np.full(len(device_models),0.)\n","total_budget = []\n","total_budget = np.full(len(device_models),0.)\n","\n","\n","\n","global_loss_epoch = 10\n","mp.set_start_method('fork')\n","number_of_iterations = ((endingTime - beginningTime)//args.trainingInterval)+1\n","number_of_training_rounds = 0 \n","for epoch in tqdm(range(args.epochs)):\n","# while global_loss_epoch > 0.06:  \n","  for i in tqdm(range(number_of_iterations)):\n","    timestamp = random.randrange(beginningTime, endingTime, args.trainingInterval)\n","    # iterate over the dataset    \n","    # start the parallelism\n","    quotient = number_of_training_rounds // args.epoch_period\n","    remainder = number_of_training_rounds % args.epoch_period \n","    processes = []\n","    \n","    weight_dict[timestamp] = []\n","\n","    # iterate over the local models\n","    for i in range(len(device_models)):\n","      # train the local model\n","      device_models[i].train()\n","        \n","      if args.featureMode == \"None\":  \n","        p = mp.Process(target=update_weights, args=(timestamp, LocalTrainData[i], device_models[i], args))\n","      else:\n","        if args.featureMode ==  \"InvContribution\" or args.featureMode ==  \"LinContribution\" :\n","          p = mp.Process(target=pertub_weights, args=(timestamp, LocalTrainData[i], device_models[i],privacy_budgets[i],args))\n","        else:\n","          if args.featureMode ==  \"Uniform\":  \n","            p = mp.Process(target=pertub_weights, args=(timestamp, LocalTrainData[i], device_models[i],privacy_budgets[i],args))\n","        p.start()\n","        \n","        processes.append(p)\n","    for p in processes:\n","      p.join()\n","    \n","    for i in range(len(device_models)):\n","      device_models[i].eval()\n","      weight_dict[timestamp].append(Pair(LocalTrainData[i].x_axis, LocalTrainData[i].y_axis,device_models[i]))  \n","    # train the global model\n","    number_of_training_rounds += 1\n","    torch.cuda.empty_cache()\n","    weights = weight_dict[timestamp]\n","    predictiondata = {}\n","    with torch.no_grad():\n","      for weight_pair in weights:\n","        sample_ID = str(weight_pair.x_axis)+\"X\"+str(weight_pair.y_axis)\n","        predictiondataset = LocalPredictionSamples.get(sample_ID)\n","        predictiondata[sample_ID]= generate_local_prediction(weight_pair,predictiondataset,args)\n","      PredictionGlobalMap = build_map(predictiondata,args)\n","    global_acc_epoch,global_loss_epoch = update_global_weights(globalModel,GlobalPredictionSamples,PredictionGlobalMap,args,adj)      \n","    global_training_loss.append(global_loss_epoch)\n","    print(\"global_loss_epoch:\",global_loss_epoch)\n","    global_training_accuracy_loss_RMSE.append(global_acc_epoch[\"RMSE\"])\n","    global_training_accuracy_loss_WMAPE.append(global_acc_epoch[\"WMAPE\"])\n","    global_training_accuracy_loss_AE.append(global_acc_epoch[\"AE\"])\n","    del PredictionGlobalMap\n","    del predictiondata\n","    gc.collect()\n","\n","global_acc_training_RMSE = sum(global_training_accuracy_loss_RMSE) / len(global_training_accuracy_loss_RMSE)\n","\n","global_acc_training_WMAPE = sum(global_training_accuracy_loss_WMAPE) / len(global_training_accuracy_loss_WMAPE)\n","global_acc_training_AE = sum(global_training_accuracy_loss_AE) / len(global_training_accuracy_loss_AE)\n","\n","\n","### testing the global model: \n","globalModel.eval() \n","global_test_output, test_acc, test_loss = test_model(globalModel, adj, GlobalTestData,args)\n","print(\"global RMSE\",test_acc[\"RMSE\"],\"global WMAPE\",test_acc[\"WMAPE\"],\"global AE\",test_acc[\"AE\"])\n","print(\"global test_loss\",test_loss) \n","\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","#epoch and loss\n","#epoch and accuracy\n","plt.subplot(3, 2, 1)\n","plt.plot(global_training_loss, color='g', label = 'training loss')\n","plt.legend(loc=\"upper left\")\n","plt.title('global loss')\n","plt.xlabel('Epoch')\n","\n","plt.subplot(3, 2, 2)\n","plt.plot(global_training_accuracy_loss_AE, color='g', label = 'training AE')\n","plt.legend(loc=\"upper left\")\n","plt.title('training AE')\n","plt.xlabel('Epoch')\n","\n","plt.subplot(3, 2, 3)\n","plt.plot(global_training_accuracy_loss_RMSE, color='g', label = 'training RMSE')\n","plt.legend(loc=\"upper left\")\n","plt.title('training RMSE')\n","plt.xlabel('Epoch')\n","\n","plt.subplot(3, 2, 4)\n","plt.plot(global_training_accuracy_loss_WMAPE, color='g', label = 'training WMAPE')\n","plt.legend(loc=\"upper left\")\n","plt.title('training WMAPE')\n","plt.xlabel('Epoch')\n"]},{"cell_type":"markdown","metadata":{"id":"IIpDGHiOAtUe"},"source":["**Save the trained model**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1562,"status":"ok","timestamp":1665941512011,"user":{"displayName":"fatima ronda","userId":"15031632683178563098"},"user_tz":240},"id":"5xjuwzr0Axlg","outputId":"391a9759-8f3f-4ff0-bf52-8959cad5561b"},"outputs":[{"name":"stdout","output_type":"stream","text":["filename is  /content/drive/MyDrive/Colab Notebooks/Models/BikeNYC/ConPrivacyGAN30.pth\n","Saving global model...\n","global saved successfully.\n"]}],"source":["import os\n","import sys\n","import torch\n","\n","sys.path.append('/content/drive/MyDrive/AdaVFL-GitHub')\n","\n","ratio = args.featureRatio\n","\n","if args.dataset == \"Yelp\":\n","  directory_name = \"/content/drive/MyDrive/Colab Notebooks/Models/Yelp/\"\n","else:\n","  directory_name = \"/content/drive/MyDrive/Colab Notebooks/Models/BikeNYC/\"\n","\n","if args.featureMode== \"None\":\n","  file_name = \"NoPrivacyGAN.pth\"\n","else:\n","  if args.featureMode== \"Uniform\":\n","    file_name = \"UniformPrivacyGAN.pth\"\n","  else:\n","    if args.featureMode== \"InvContribution\":\n","      file_name = 'InvConPrivacyGAN{}.pth'.format(ratio)\n","    else:\n","      if args.featureMode== \"LinContribution\":\n","        file_name = 'LinConPrivacyGAN{}.pth'.format(ratio)\n","\n","print('Saving global model...')\n","torch.save(globalModel.state_dict(), directory_name+file_name)\n","print('global saved successfully.')\n"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1bVeFt6EljbQruYjV1XLo9HSRsK4HbTlC","authorship_tag":"ABX9TyNODmO507j33XY1eUgbu7NO"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}